# Pythonを用いた初心者向けAI実践講座(中級編) 11/16 配布資料

## 2-1 確率統計の基礎

　第2章では、ベイズ機械学習を取り扱う。あまり聞き慣れない言葉かもしれないが、PRML等
の機械学習の名著を読む際には必須の知識であり、応用においても、近年の話題の深層学習(DeepLearning)と
双璧をなすツールである。(これら2つの間の密接な関係を示した論文も多く存在している。)
まず本節では、ベイズ機械学習の前提知識となる「確率統計」という数学の分野を扱う。


### 2-1-1 事象族、確率、確率分布

確率的に取り扱いたいものがあったとして、そのシステムを次のように表現するとする。

$$\Omega = \{w_1,w_2,...\} \tag{2.1.1}$$

$\Omega$は標本空間、$w_1,...$は根元事象と呼ばれている。非常に抽象的だが、サイコロを例として考えると、サイコロを1回ふるという問題を考えるときは、次のような表記をすると単に言っているだけである。

$$\Omega = \{w_1,...,w_6\} = \bigl\{1が出る, ... , 6が出る\bigl\}$$

次にサイコロの目の出方に対して何に注目するか(これをここでは興味と呼ぶことにする)を考えて、以下のような表を作成する。

(興味1) 奇数?or偶数?

|$F'$|$\{w_1,w_3,w_5\}$|$\{w_2,w_4,w_6\}$|
|:----:|:----:|:----:|
|$P$|1/2|1/2|

:サイコロの確率分布1

$$F' = \{\ \{w_1,w_3,w_5\}, \ \{w_2,w_4,w_6\}\ \} \tag{2.1.2}$$

この事象族$F'$の要素(集合)に対してある数値を返すものを「確率」と呼ぶ。(集合から数値への写像とも言える。$P\bigl(\{w_1,w_3,w_5\}\bigl) = 1/2$)
また、「事象族$F'$」と「確率$P$」を合わせたものを「確率分布」と呼ぶ。(要するに表1が確率分布である。)よって興味が変われば確率分布もそれに対応して変化する。

(興味2) 1?or1以外?

|$F'$|$\{w_1\}$|$\{w_2,w_3,w_4,w_5,w_6\}$|
|:----:|:----:|:----:|
|$P$|1/6|5/6|

:サイコロの確率分布2


(1)赤玉7個、白玉3個入っている箱から、玉を2回取り出す。(1回玉を取り出してから、それを箱に「戻し」、もう一度取り出す。)

1. 標本空間$\Omega$を求めよ。

2. 「玉の色の組み合わせ」に興味がある場合の事象族$F_1$を定め、確率分布を求めよ。

3. 「白玉が出る or 出ない」に興味がある場合の事象族$F_2$を定め、確率分布を求めよ。

### 2-1-2 確率変数

次のような確率分布を考える。

|$F'$|$\cdots  A_i \cdots$|
|:-----:|:-----:|
|$P$|$\cdots  p_i \cdots$|

:一般的な確率分布$P(A_i) = P(\{w\in A_i\}) = p_i$

この分布において、$w\in A_i$のとき($A_i$が起こったとき)、数値$x_i$が定まるという状況を考える。つまり、

$$X(w) = \begin{cases}
            x_1 & (w\in A_1) \\
            \ \vdots & \qquad\vdots \\
            x_n & (w \in A_n)
        \end{cases} \tag{2.1.3}$$

である。この$X$を確率変数と呼ぶ。(ここで示したように、確率変数は厳密には、根元事象の関数である。)確率変数を含めた表は以下のようになる。

|$F'$|$\cdots  A_i \cdots$|
|:-----:|:-----:|
|$X$|$\cdots  x_i \cdots$|
|$P$|$\cdots  p_i \cdots$|

具体的な例は、次節の練習問題で触れる。

### 2-1-3 期待値、分散

期待値(平均)を次のように定義する。

$$E[X] = \sum_{i=1}^{n}x_ip_i \tag{2.1.4}$$

この量が実際に妥当なものなのかを次の問題で考える。

(2)次の確率分布について考える。

|$F'$|赤|青|黄|
|:-----:|:-----:|:-----:|:-----:|
|$X$|$10$|$100$|$1000$|
|$P$|$6/10$|$3/10$|$1/10$|

:確率分布

以下のようなケースを考え、式()で定義した平均の妥当性を調べなさい。(詳細未定)

　分散については次のように定義する。

$$Var(X) = E\bigl[(X-E[X])^2\bigl] \tag{2.1.5}$$

また、単位を確率変数$X$に揃えたものを標準偏差といい、分散を用いて

$$\sqrt{Var(x)} = \sqrt{E\bigl[(X-E[X])^2\bigl]} \tag{2.1.6}$$

と定義する。

(3)(2)の確率分布についてその分散と標準偏差を求めなさい。

### (重要) 期待値、分散の性質1

$$1. \ E[aX + b] = aE[X] + b \tag{2.1.7}$$
$$2. \ Var[aX + b] = a^2Var[x] \tag{2.1.8}$$
$$3. \ Var[X] = E[X^2] - E[X]^2 \tag{2.1.9}$$


### 2-1-4 同時分布、周辺確率

ある対象の2つの事象族$F_A,F_B$を考える。

$$F_A = \{A_1,A_2,\cdots,A_n\}$$
$$F_B = \{B_1,B_2,\cdots,B_m\}$$

この事象族に対して以下のような確率分布を考えたとき、この確率分布を「同時(確率)分布」と呼ぶ。

|$F_B \ / \ F_A$|$A_1\qquad \cdots\cdots \qquad A_n$|
|:-----:|:---------:|
|$B_1$|$P(A_1\cap B_1)\cdots\cdots P(A_n \cap B_1)$|
|$\vdots$||
|$B_n$|$P(A_1\cap B_m)\cdots\cdots P(A_n\cap B_m)$|

:同時分布$P_{AB}(A_i\cap B_j)$

例えばサイコロの同時分布の例として以下のようなものを考えることができる。

|$F_B / F_A$|$\{偶数\}$|$\{奇数\}$|
|:-----:|:-----:|:-----:|
|$\{3以下\}$|$1/6$|$2/6$|
|$\{4以上\}$|$2/6$|$1/6$|

:同時分布の例

同時分布がわかれば、対象に対してあらゆる情報を持っているということになる。例えば、表5に対して$i$行目の要素の和をとり、

$$P_{AB}(A_1\cap B_i) + P_{AB}(A_2\cap B_i) + \cdots + P_{AB}(A_n\cap B_i) $$
$$=P_{AB}\bigl(\{A_1\cap B_i\}\cup\{A_2\cap B_i\}\cup...\cup\{A_n\cap B_i\}\bigl)$$
$$=P_{AB}(B_i)\equiv P_B(B_i)$$

とすることができる。これを「周辺分布」と呼び、以下のように定義され、このような操作を「周辺化」と呼ぶ。

$$P_A(A_i) \equiv \sum_{j=1}^{m}P_{AB}(A_i\cap B_j) \tag{2.1.10}$$
$$P_B(B_j) \equiv \sum_{i=1}^{n}P_{AB}(A_i\cap B_j) \tag{2.1.11}$$

表5に周辺分布の情報を入れると以下のようになる。

|$F_B \ / \ F_A$|$A_1\qquad \cdots\cdots \qquad A_n$||
|:-----:|:---------:|:-----:|
|$B_1$|$P(A_1\cap B_1)\cdots\cdots P(A_n \cap B_1)$|$P_B(B_1)$|
|$\vdots$|||
|$B_n$|$P(A_1\cap B_m)\cdots\cdots P(A_n\cap B_m)$|$P_B(B_m)$|
||$P_A(A_1)\quad \cdots\cdots \quad P_A(A_n)$|

:同時分布と周辺分布

(補足)
後ほど出てくるが、例えば機械学習であれば、$p(X,y,w)$といった同時分布を学習の際に用いて(構築して)、予測する際は学習に用いた重み$w$を周辺化によって同時分布から削除するといった操作をよく行う。このように注目している確率変数以外のものを周辺化によって消去する操作として周辺化は有用である。(ちなみに、実際は同時分布$p(X,y,w)$を構築することは少なく、ベイズの定理を
用いて学習則を記述することがほとんどである。しかし、同時分布には対象の情報が
全て含まれているわけなので、これを求めたいというモチベーションは当然機械学習で応用されても同じである。同時分布を直接構築する方法はベイズ機械学習の分野では「生成モデル」と呼ばれていて、これを求められれば、学習データから未知のデータを生成するといったことが可能になる。生成モデルについては2-3節、確率的生成モデルを参照。)


### 2-1-5 独立性

以下の性質が成り立つとき、$F_A, F_B$は「独立」と呼ばれている。

$$P_{AB}(A_i\cap B_j) = P_A(A_i)P_B(B_j) \tag{2.1.12}$$

(4)1つのサイコロを続けて2回投げる試行を行う。以下のように事象族$F_1,F_2$を定めるとき、2つの事象族が独立であるかどうかを判定せよ。

$F_1 = \{A_1,A_2\}, F_2 = \{B_1,B_2\}$とする。

1. $A_1 = \{1回目のサイコロの目が偶数\}, A_2 = \{1回目のサイコロの目が奇数\} \\ B_1 = \{1回目と2回目で同じ目\}, \quad \quad B_2 = \{1回目と2回目で違う目\}$

2. $A_1 = \{1回目のサイコロの目が偶数\}, A_2 = \{1回目のサイコロの目が奇数\} \\ B_1 = \{2回の試行で最低1回1が出る\}, B_2 = \{1回も1が出ない\}$


### (重要) 期待値、分散の性質2

$$1.\ E_{XY}[X+Y] = E_X[X] + E_Y[Y] \tag{2.1.13}$$
$$2.\ V_{XY}[X+Y] = V_{X}[X] + V_Y[Y] + 2(E_{XY}[XY] - E_X[X]E_Y[Y]) \tag{2.1.14}$$

性質2の右辺第3項目は「共分散」と呼ばれており、以下のように定義される。

$$Cov[X,Y] = E_{XY}[XY] - E_X[X]E_Y[Y] \tag{2.1.15}$$

上記の独立性の性質を使うと、

$$E_{XY}[XY] = E_X[X]E_Y[Y] \tag{2.1.16}$$

が成立することがわかるため、$Cov_{XY}[X,Y]=0$となる。このような状態を
「$X$と$Y$は無相関である」という。(注意 独立なら無相関である。無相関なら独立なのではない。)

(補足) 上記説明は結構乱暴なもので、これらの性質を厳密に示していくには、
多次元確率分布の定義をして、性質を見ていかないといけない。その中でいくつかの重要な事項(確率分布の畳み込みや大数の法則)が導かれるのでそれはColumnにゆずった。ここでは、分散には単純な線形性が成り立たないことと、
共分散を抑えておけば十分である。

### 2-1-6 条件付き確率

表7を再掲する。

|$F_B \ / \ F_A$|$A_1\qquad \cdots\cdots \qquad A_n$||
|:-----:|:---------:|:-----:|
|$B_1$|$P(A_1\cap B_1)\cdots\cdots P(A_n \cap B_1)$|$P_B(B_1)$|
|$\vdots$|||
|$B_j$|$P(A_1\cap B_j)\cdots\cdots P(A_n \cap B_j)$|$P_B(B_j)$|
|$\vdots$|||
|$B_n$|$P(A_1\cap B_m)\cdots\cdots P(A_n\cap B_m)$|$P_B(B_m)$|
||$P_A(A_1)\quad \cdots\cdots \quad P_A(A_n)$|

:同時分布と周辺分布

この表において$B_j$が起こったとき($B_j$の情報が得られたとき)、次のような確率を考え、これを「条件付き確率」と呼ぶ。

$$P(A_i|B_j) \equiv \frac{P(A_i\cap B_j)}{P(B_j)} \tag{2.1.17}$$

(こうして定義した量が確率分布の性質を満たしているかは別途証明が必要だが、
そこまで難しくないので割愛。)

また、$A$と$B$が独立なとき、$P(A_i\cap B_j)=P(A_i)P(B_j)$なので、

$$P(A_i|B_j) = \frac{P(A_i\cap B_j)}{P(B_j)} = \frac{P(A_i)P(B_j)}{P(B_j)} = P(A_i) \tag{2.1.18}$$

となる。(この性質から、独立性の定義が妥当だったとも言える。)

(5)奇数の目の部分に黒いシールが、偶数の目の部分に白いシールが貼られているサイコロを1つ投げる試行を考える。このサイコロを投げて「黒」が出たときの確率分布を考えなさい。


### 2-1-7 ベイズの定理

2-1-6で定義した条件付き確率$P(A_i|B_j)$は、「確率$P(A_i)$が、$B_j$が与えられたことによって、$P(A_i|B_j)$に更新された」と捉えることができる。式()に戻り、条件付き確率を少し変形すると、

$$P(A_i|B_j) = \frac{P(A_i\cap B_j)}{P(B_j)} = \frac{P(B_j\cap A_i)}{P(B_j)} = \frac{P(B_j|A_i)P(A_i)}{P(B_j)}$$ 
$$ = \frac{P(B_j|A_i)}{P(B_j)}P(A_i) \tag{2.1.19}$$

となり、これを「ベイズの定理」という。これは上記の、「確率$P(A_i)$が、$B_j$が与えられたことによって、$P(A_i|B_j)$に更新された」という表現を数式の上で明らかにするために、$P(A_i)$を陽に表した式変形とも言える。
後の章で扱うが、この$P(A_i)$を「事前分布」、更新された$P(A_i|B_j)$を「事後分布」と呼ぶ。ベイズ機械学習は単に、このベイズの定理を使って事前分布の情報を更新し、データに対する予測精度を上げていく手法であり、この更新プロセスを「学習」と呼んでいる。このベイズの定理を用いた「知識(情報)の更新」は応用上非常に強力で、機械学習のみならず様々な工学分野で応用されている。

ちなみに式()に現れている$P(B_j|A_i)$は、この式の中では、「尤度関数」と呼ばれている。また、さらに$P(A_i)$を陽に表すために、周辺分布の性質を使って以下のような表現をしてあることが多い。

$$P(A_i|B_j) = \frac{P(B_j|A_i)}{P(B_j)}P(A_i) = \frac{P(B_j|A_i)P(A_i)}{\sum_{i}P(B_j|A_i)P(A_i)} \tag{2.1.20}$$

(ただし、式変形した結果からわかるように、単にこれは分子の値を足し合わせて規格化しているだけなので、実際は特に気にせず分子だけ計算して問題に合わせて全確率の性質を使って規格化すればいい。)

(6)ある海域に船(潜水艦)が沈没している。この船を引き揚げるために、以下のような方策をとる。まず、その海域をグリッドに分割し、各グリッドに船が沈んでいる確率を適当に割り振る。次に、実際に1つのグリットにおいて船の捜索を行い、
もし船が見つからなかったら、そのグリッドに船が沈んでいる確率を更新し、
同様に、他のグリッドに船が沈んでいる確率を(全確率の和が1であるように)均等に変化させる。あるグリットに注目し、

$$A = \{そのグリッドに船が沈んでいる\}$$
$$B = \{そのグリッドにおける捜索の結果、船が発見される\}$$

という2つの事象を考える。さらに$p=P(A), q = P(B|A)$は予め決まっているものとする。このとき、上記の確率の更新則を決めなさい。


### 2-1-8 確率密度関数

連続的な確率分布を考える(例 正規分布)。この場合の標本空間$\Omega$は、

$$\Omega = \{R(実数)上に値をとる\}$$

この場合の$F',P$を次のように考える。連続的な値をとる場合はとりうる事象が
無限通りになって議論ができないので、(微小)区間$\Delta x$を考え、事象族$F'$を、

$$F' = \{\cdots,\{[x + \Delta x) にある\},\cdots \}$$

とする。さらに、これに対する確率$P$を次のように決める。

$$P\bigl(\{[x + \Delta x)\}\bigl) = p(x)\Delta x \tag{2.1.21}$$

つまり確率分布は、

|$F'$|$\cdots ,\{[x + \Delta x)\}, \cdots$|
|:----:|:----:|:----:|
|$P$|$\cdots ,p(x)\Delta x, \cdots$|


この$p(x)$を確率密度関数と呼ぶ。(注意 確率$P$と確率密度関数$p(x)$は上記のように別物である。)

上記のような定義から、確率密度関数は次のような性質が要請される。

$$1.\ p(x) \geq 0 \tag{2.1.22}$$
$$2.\ P\bigl(\{[a,b]\}\bigl) = \int_{a}^{b}p(x)dx \tag{2.1.23}$$
$$3.\ \int_{-\infty}^{\infty}p(x)dx = 1 (規格化条件) \tag{2.1.24}$$


### 2-1-9 有名な確率密度関数

ベイズ機械学習においてよく出る確率分布の確率密度関数についていくつか紹介する。

### ベルヌーイ分布

$$ Bern(x) = \mu^x(1-\mu)^{1-x} \tag{2.1.25}$$

この分布の平均と分散は、

$$E[x] = \mu\tag{2.1.26}$$
$$var[x] = \mu(1-\mu)\tag{2.1.27}$$

### カテゴリー分布

$$Cat(\vec{s}) = \prod^{K}_{k=1}\pi_k^{s_k}\tag{2.1.28}$$

$$E[s_k] = \pi_k\tag{2.1.29}$$
$$var[x] = \mu(1-\mu) \tag{2.1.30}$$


### ポアソン分布

$$Poi(x) = \frac{\lambda^x}{x!}e^{- \lambda} \tag{2.1.31}$$

$$E[x] = \lambda \tag{2.1.32}$$
$$var[x] = \lambda \tag{2.1.33}$$


### 正規分布(ガウス分布)

$$p(x) = \frac{1}{\sqrt{2\pi \sigma^2}}\exp\biggl[-\frac{1}{2\sigma^2}(x-\mu)^2\biggl]\tag{2.1.34}$$

$$E[x] = \mu\tag{2.1.35}$$
$$var[x] = \sigma^2\tag{2.1.36}$$


### (おまけ) ベータ分布

$$ Beta(\mu) = \frac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1} \tag{2.1.37}$$


$$E[x] = \frac{a}{a+b} \tag{2.1.38}$$
$$var[x] = \frac{ab}{(a+b)^2(a+b+1)} \tag{2.1.39}$$


### (おまけ2) スチューデントの$t$分布

$$St(x) = \frac{\Gamma(v/2 + 1/2)}{\Gamma(v/2)}\biggl(\frac{\lambda}{\pi v}\biggl)^{1/2}\biggl[1 + \frac{\lambda(x-\mu)^2}{v}\biggl]^{-v/2-1/2} \tag{2.1.40}$$


(7)確率密度関数が、

$$p(x) = \frac{1}{\sqrt{2\pi \sigma^2}}\exp\biggl[-\frac{1}{2\sigma^2}(x-\mu)^2\biggl] \tag{2.1.41}$$

で表される正規分布について、平均と分散を求めよ。

