# Pythonを用いた初心者向けAI実践講座(中級編) 12/7 配布資料

## 2-2 ベイズ推論の基礎

### 2-2-1 ベイズ推論における学習、予測

　ベイス推論では、パラメータ（重み）も不確実性をもつ確率変数として捉え、
次の手順で問題を解くことが多い。

1. 問題に合わせて、適切な尤度関数$p(D|\vec{w})$を設定する。（モデル化）
2. 尤度関数にあった共役事前分布$p(\vec{w})$を選ぶ。
3. ベイスの定理を用いて、事後分布$p(\vec{w}|D)$を解析的に求める。（学習）
4. 事後分布を用いて、予測分布$p(x_*|D)$を計算する。（予測）

### $Step1.$　尤度関数$p(D|\vec{x})$の設計（モデル化）
　解きたい問題に対して可視化などを行い、ある程度妥当であると思われる尤度関数を設定する。
線形回帰など一般的にはガウス分布を用いることが多いが、カウントデータ（非負）ならポアソン分布、周期性をもつ分布はフォンミーゼス分布（PRML2章）など、
データの分布に合わせた確率分布を選ぶことが望ましい。

### $Step2.$　共役事前分布$p(\vec{w})$の設定
　設定した尤度関数の共役事前分布$p(\vec{w})$を選ぶ。「共役事前分布」はベイズの
定理ととても相性がよく、ベイズの定理を適用してもその分布の形が変わらない
分布である。大抵は尤度関数と1対1の関係で決まっているので、この$Step2.$は
すぐに終わる。

### $Step3.$ 　学習
ベイスの定理を用いて以下の事後分布$p(\vec{w}|D)$を計算する

$$p(\vec{w}|D)=\frac{p(D|\vec{w})p(\vec{w})}{p(D)} \tag{2.2.1}$$

### $Step4.$　 予測
未観測のデータ$x_*$に対して以下の予測分布を計算する。

$$p(x_*|D)=\int p(x_*|\vec{w})p(\vec{w}|D) \ d\vec{w} \tag{2.2.2}$$

これは予測に際して必要ない$\vec{w}$について積分除去を行ったものと考えることができる。
また、事後分布とは異なり、一般的には予測分布は共役事前分布の形になるとは
限らない。

（1）尤度関数としてベルヌーイ分布

$$p(x|\mu)=Bern(x|\mu) \tag{2.2.3}$$

でモデル化できる問題において、$\mu$の分布を訓練データ$x_n$から推論
する更新則を決定せよ。また未観測の値$x_*\in 0,1$に対する予測分布を計算せよ。

(略解) ベルヌーイ分布の共役事前分布はベータ分布なので、ベイズの定理を用いて解析的に計算を行う。答えは、

ベータ分布のパラメータ$a,b$に関して、

$$a \rightarrow \sum_{n=1}^N x_n + a \tag{2.2.4}$$
$$b \rightarrow N - \sum_{n=1}^N x_n + b \tag{2.2.5}$$

（2）線形回帰  $y_n=\vec{w}^Tx_n + \epsilon_n$ についてモデル$p(y_n|\vec{x}_n,\vec{w})$の
構築を行い、事後分布、予測分布を計算せよ。

(略解) 学習する重み$\vec{w}$と、ノイズ成分$\epsilon_n$がそれぞれ以下のガウス分布に従っていると仮定する。

$$p(\vec{w}) = N(\vec{w} | \vec{m}, \Lambda^{-1}) \tag{2.2.6}$$
$$p(\epsilon_n) = N(\epsilon_n | 0, \lambda^{-1}) \tag{2.2.7}$$

次に以下のようにベイズの定理を使って事後分布を解析的に求める。

$$p(\vec{w} | Y, X) = \frac{p(Y|X, \vec{w})p(\vec{w})}{p(Y|X)} = \frac{\prod_{n=1}^{N}p(y_n|x_n, \vec{w})p(\vec{w})}{p(Y|X)} $$
$$\propto p(\vec{w})\prod_{n=1}^{N}p(y_n|x_n, \vec{w})=\cdots \tag{2.2.8}$$


答えは、事後分布の平均$\hat{\vec{m}}$、精度$\hat{\Lambda}$に関して、

$$\hat{\vec{m}} = \hat{\Lambda}^{-1}(\lambda\sum_{n=1}^{N}y_n\vec{x}_n + \Lambda \vec{m}) \tag{2.2.9}$$
$$\hat{\Lambda} = \lambda\sum_{n=1}^{N}\vec{x}_n\vec{x}_n^T + \Lambda \tag{2.2.10}$$

### 2-2-2(補足) モデルエビデンス（周辺尤度）
ベイズの定理を変形して、

$$p(D)=\frac{p(D|\vec{w})p(\vec{w})}{p(\vec{w}|D)} \tag{2.2.11}$$

と表す。このとき、$p(D)$を周辺尤度（モデルエビデンス）と呼ぶ。これは
モデルのデータ生成確率と解釈することができ、この値を複数のモデル間で
比較することで最適なモデルの選択を行うことができる。

\clearpage

(解答)

\clearpage

(解答)

\clearpage

(解答)

\clearpage

(解答)




