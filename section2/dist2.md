# 第２章　ベイズ機械学習

## 2-1 確率統計の基礎

### 確率と確率変数

### 確率分布

### 周辺確率

### 期待値、分散

### 独立性

### 条件付き確率

### ベイズの定理

### 確率密度関数

### 有名な確率分布


\clearpage

## 2-2 ベイズ推論の基礎

### 2-2-1 ベイズ推論における学習、予測

ベイス推論では、パラメータ（重み）も不確実性をもつ確率変数として捉え、
次の手順で問題を解くことが多い。

1. 問題に合わせて、適切な尤度関数$p(D|\vec{w})$を設定する。（モデル化）
2. 尤度関数にあった共役事前分布$p(\vec{w})$を選ぶ。
3. ベイスの定理を用いて、事後分布$p(\vec{w}|D)$を解析的に求める。（学習）
4. 事後分布を用いて、予測分布$p(x_*|D)$を計算する。（予測）

### $Step1.$　尤度関数$p(D|\vec{x})$の設計（モデル化）
解きたい問題に対して可視化などを行い、ある程度妥当であると思われる尤度関数を設定する。
線形回帰など一般的にはガウス分布を用いることが多いが、カウントデータ（非負）ならポアソン分布、周期性をもつ分布はフォンミーゼス分布（PRML2章）など、
データの分布に合わせた確率分布を選ぶことが望ましい。

### $Step2.$　共役事前分布$p(\vec{w})$の設定
設定した尤度関数の共役事前分布$p(\vec{w})$を選ぶ。「共役事前分布」はベイズの
定理ととても相性がよく、ベイズの定理を適用してもその分布の形が変わらない
分布である。大抵は尤度関数と1対1の関係で決まっているので、この$Step2.$は
すぐに終わる。

### $Step3.$ 　学習
ベイスの定理を用いて以下の事後分布$p(\vec{w}|D)$を計算する

$$p(\vec{w}|D)=\frac{p(D|\vec{w})p(\vec{w})}{p(D)}$$

### $Step4.$　 予測
未観測のデータ$x_*$に対して以下の予測分布を計算する。

$$p(x_*|D)=\int p(x_*|\vec{w})p(\vec{w}|D) \ d\vec{w}$$

これは予測に際して必要ない$\vec{w}$について積分除去を行ったものと考えることができる。
また、事後分布とは異なり、一般的には予測分布は共役事前分布の形になるとは
限らない。

（1）尤度関数としてベルヌーイ分布

$$p(x|\mu)=Bern(x|\mu)$$

でモデル化できる問題において、$\mu$の分布を訓練データ$x_n$から推論
せよ。また未観測の値$x_*\in 0,1$に対する予測分布を計算せよ。

（2）線形回帰  $y_n=\vec{w}^Tx_n + \epsilon_n$ についてモデル$p(y_n|\vec{x}_n,\vec{w})$の
構築を行い、事後分布、予測分布を計算せよ。

### 2-2-2 モデルエビデンス（周辺尤度）
ベイズの定理を変形して、

$$p(D)=\frac{p(D|\vec{w})p(\vec{w})}{p(\vec{w}|D)}$$

と表す。このとき、$p(D)$を周辺尤度（モデルエビデンス）と呼ぶ。これは
モデルのデータ生成確率と解釈することができ、この値を複数のモデル間で
比較することで最適なモデルの選択を行うことができる。


## 2-3 確率的生成モデル
現実の問題では、データを生成する分布は複雑で1つの確率分布で取り扱えるケースは多くない。複数の分布をデータの生成過程を仮定しながら組み合わせて全体のモデル
（同時分布）を作り、そこから事後分布、予測分布を計算する手法を「確率的
生成モデル」と呼び、確率分布を複数組み合わせてできたモデルを「混合モデル」と呼ぶ。

### 2-3-1 混合モデルの構築

多峰性をもつデータに関してのクラスタリングを考える。データを表現するため
のモデルを構築する要件定義として例えば以下の過程を考える。

1. K個のクラスタは混合比率$\pi=(\pi_1,...,\pi_K)$で分布上に存在し、
$\pi$は事前分布$p(\pi)$から生成される。
2. それぞれのクラスタ自身の持つパラメータ$\theta_k$が事前分布$p(\theta_k)$から生成される。
3. データ点$x_n$が$K$個ある分布うちのどれかから生成されるとし、
$x_n$に対応するクラスタの割り当てを$s_n$をする。この$s_n$は
比率$\pi$によって決まるとし、$s_n$の生成する分布を
$p(s_n|\vec{\pi})$とする。

4. $s_n$によって選択された$k$番目の確率分布
$p(\vec{x_n}|\vec{\theta_k})$からデータ$x_n$が生成される。

これら全ての確率分布をデータ生成順に組み合わせ、$N$個のデータに関して同時分布を考えると以下のようになる。

$$p(X,S,\Theta,\vec{\pi}) = p(X|S,\Theta)p(S|\vec{\pi})p(\Theta)p(\vec{\pi}) 
= \bigl\{\prod_{n=1}^{N}p(\vec{x_n}|\vec{s_n},\Theta)p(\vec{s_n}|\vec{\pi})\bigl\} \bigl\{\prod_{k=1}^{K}p(\vec{\theta_k})\bigl\}p(\vec{\pi})$$

実際に問題を解く際には、$p(X|S,\Theta)$,$p(\Theta)$は問題設定に
応じて決め、（クラスタリングの場合は）$s_n$をサンプリングする分布として
以下のカテゴリ分布、

$$p(\vec{s_n}|\vec{\pi}) = Cat(\vec{s_n}|\vec{\pi}) = \prod_{k=1}^K \pi_{k}^{s_{n,k}} $$

$\pi$をサンプリングする分布としてカテゴリ分布の共役事前分布である
$Dirichlet$分布を選ぶことが多い。

$$p(\vec{\pi})=Dir(\vec{\pi}|\vec{\alpha})$$ 

また$s_n$は直接は観測されないが、$x_n$を生成する$K$個の分布のうち1つを
選択するという意味で、$x_n$を発生させる確率分布を潜在的に決めている
確率変数であると言える。このため$s_n$は潜在変数と呼ばれている。

（1）あるクラスタ$k$に対する観測モデルとしてポアソン分布を採用し、
混合モデルを構築せよ。

### 2-3-2 混合モデルの推論

この同時分布から事後分布$p(S,\Theta,\vec{\pi}|X)$,クラスタ$S$の推定$p(S|X)$が可能であるが、いずれの計算も

$$p(X)=\sum_{S}\iint p(X,S,\Theta,\pi) d\Theta d\pi \\
=\sum_{s}p(X,S)$$

$$p(S|X) = \iint p(S,\Theta,\pi|X) d\Theta d\pi$$

の計算が発生してしまい、解析的に解くことがほぼ不可能になる。次章で、この問題
をある程度解消して近似的に解を出す方法を説明する。



## 2-4 近似推論

事後分布、周辺尤度、予測分布など問題によっては解析的に解くことが難しい
ものに関しては、近似的に解を求めることが多い。近似手法は大きく分けると、サンプリング、変分法に大別される。

### 2-4-1 ギブスサンプリング

分布全体の解析的な把握が難しい場合、期待値等の分布に関する部分的な統計
量を解析することは重要である。そのような各種統計量を得たい場合、分布
から複数の実現値をサンプリングし、その実現値を元に計算を行うことが
有効的である。

$$z_1^{(i)},z_2^{(i)},z_3^{(i)} \sim \ p(z_1,z_2,z_3)$$

混合モデル等、複雑なモデルに関しては全てのサンプルを上記のように同時に
サンプルすることは難しいため、ギブスサンプリングという手法を用いて以下の
ようにサンプリングを行う。

$$z_1^{(i)} \sim \ p(z_1|z_2^{(i-1)},z_3^{(i-1)})$$
$$z_2^{(i)} \sim \ p(z_2|z_1^{(i)},z_3^{(i-1)})$$
$$z_3^{(i)} \sim \ p(z_3|z_1^{(i)},z_2^{(i)})$$

この手法はMCMC（マルコフ連鎖モンテカルロ法）の手法の一つに分類されており、
サンプル数が十分に多い場合、繰り返しで得られた$z_k$は真の事後分布から
得られたものであると理論的に保証されている。（$Column$参照）


（1）ギブスサンプリングを用いて、（）で求めたポアソン混合モデルの
事後分布$p(S,\vec{\lambda},\vec{\pi}|X)$からサンプリングを行う
アルゴリズムを導け。混合分布では以下のように、潜在変数とパラメータを次の
ように分けてサンプリングすると簡単な確率分布が得られることが知られている。

$$S \sim p(S|X,\vec{\lambda},\vec{\pi}), \quad \vec{\lambda}, \ \vec{\pi} \sim p(\vec{\lambda},\vec{\pi}|X,S)$$

### 2-4-2 平均場近似(変分推論)

複雑な分布を最適化問題を解くことによってより簡単な近似分布で表現する手法を「変分推論」、「変分近似」と呼ぶ。事後分布は解析的に解けなくなる状況に陥ることがあるため、確率変数に特定の制約を付けた上で事後分布を近似する。

最適化にはKLダイバージェンスを使い、最小化問題として以下のように定式化される。


$$q_{opt} = argmin_q KL[q(z_1,z_2,z_3) | \ p(z_1,z_2,z_3)]$$

ここで、解が$q_{opt}(z_1,z_2,z_3) = p(z_1,z_2,z_3)$とならないように$q$に制約
をつける手法として、各確率変数に独立性の仮定をおく。

$$p(z_1,z_2,z_3) \approx q(z_1)q(z_2)q(z_3)$$

これを「平均場近似」と呼ぶ。

（2）平均場近似を用いて、（）で求めたポアソン混合モデルの変分推論アルゴリズム
を導出せよ。ただし、事後分布$p(S,\vec{\lambda},\vec{\pi}|X)$
の潜在変数とパラメータを以下のように分けて近似せよ。

$$p(S,\vec{\lambda},\vec{\pi}|X) \approx q(S)q(\vec{\lambda},\vec{\pi})$$


## 2-5 ガウス混合モデルと教師なし学習

### 2-5-1 潜在変数とガウス混合モデル 

2-3節からわかるように、複雑なモデルの定式化の際に「潜在変数」を取り入れること
で問題が簡単になることがある。潜在変数を確率モデルの中に取り入れることを陽に
表すと次のように定式化される。

$$p(x) = \sum_{z}p(x,z) =\sum_{z}p(x|z)p(z) $$

式()を用いて混合ガウス分布の密度関数$p(x)$を求めると、

$$p(x) = \sum_{z}p(x|z)p(z) = \sum_{k=1}^{K}\pi_kN(x|\mu_k,\Sigma_k)$$

となる。

(1) 混合ガウス分布の密度関数$p(x)$が式()で表されることを示せ。

### 2-5-2 EMアルゴリズム(Expectation-Maximization Algorithm)

潜在変数が含まれる最尤推定の問題で使われる最適化アルゴリズム。

2-5-1で求めた$p(x)$の対数尤度関数は以下のようになる。

$$\ln p(X|\pi,\mu,\Sigma) = \sum_{n=1}^{N}\ln\Bigl\{\sum_{n=1}^{N}\pi_k N(x_n|\mu_k,\Sigma_k)\Bigl\}$$

このような潜在変数が含まれる尤度関数の最尤推定では、「EMアルゴリズム」と呼ばれる手法を使うと効率よく解を求められることが知られている。
EMアルゴリズムは以下の4つのステップからなる。

1. $\mu_k,\Sigma_k,\pi_k$を初期化し、対数尤度()の初期値を計算する。
2. (Eステップ) 1の値を用いて以下の「負担率」を計算する。

$$\gamma_k = \frac{\pi_k N(x_n|\mu_k,\Sigma_k)}{\sum_{j=1}^{K}\pi_j N(x_n|\mu_j,\Sigma_j)}$$

3. (Mステップ) 2で求めた負担率を用いて、次式で$\mu_k,\Sigma_k,\pi_k$を再計算する。

$$\mu_k^{new} = \frac{1}{N_k}\sum_{n=1}^{N}\gamma(z_{nk})\vec{x}_n$$

$$\Sigma_k^{new} = \frac{1}{N_k}\sum_{n=1}^{N}\gamma(z_{nk})(\vec{x}_n - \mu_k^{new})(x_n - \mu_k^{new})^T$$

$$\pi_k^{new} = \frac{N_k}{N}, \quad N_k = \sum_{n=1}^{N}\gamma(z_{nk})$$

4. $\mu_k^{new},\Sigma_k^{new},\pi_k^{new}$で対数尤度()を計算。対数尤度、もしくはパラメータの値の変化を見て収束性を確認。収束していなければ、
2に戻る。

なお、上記の更新ステップでは対数尤度関数は必ず増加することが保証されている。(Column)

