# 第1章　パターン認識の基礎

## 1-1 線形代数

　線形代数は、機械学習だけでなくありとあらゆる分野（工学、経済学等）で
現れる分野である。機械学習の分野では主に、複雑なデータを表現するための
道具として使われることが多い。1-1では準備体操として
行列の基本的な演算と便利な性質を見ていく。

### 1-1-1 ベクトル、行列とは

　ベクトルの例

$$
\vec{a} = \begin{pmatrix}
1  \\
2 \\
3
\end{pmatrix}, \quad
\vec{e} = \begin{pmatrix}
1  \\
0 \\
0
\end{pmatrix},
\tag{1.1.1}$$

　行列の例

$$
A = \begin{pmatrix}
1 & 2 \\
3 & 4
\end{pmatrix}, \quad
B = \begin{pmatrix}
2 & 3 \\
4 & 5
\end{pmatrix}, \quad
I = \begin{pmatrix}
1 & 0 & 0 \\ 
0 & 1 & 0 \\
0 & 0 & 1
\end{pmatrix}
\tag{1.1.2}$$

### 1-1-2 基本的な演算

　行列の和

$$A + B = \begin{pmatrix}
1 & 2 \\
3 & 4
\end{pmatrix} 
+ \begin{pmatrix}
2 & 3 \\
4 & 5
\end{pmatrix}
= \begin{pmatrix}
3 & 5 \\
7 & 9
\end{pmatrix}\tag{1.1.3}$$

　行列の積

$$AB = \begin{pmatrix}
1 & 2 \\
3 & 4
\end{pmatrix} 
\begin{pmatrix}
2 & 3 \\
4 & 5
\end{pmatrix}
= \begin{pmatrix}
1\times2 + 2\times4 & 1\times3 + 2\times5\\
3\times2+4\times4 & 3\times3+4\times5
\end{pmatrix}
$$
$$
= \begin{pmatrix}
10 & 13 \\
22 & 29
\end{pmatrix}
\tag{1.1.4}$$

(注意) 一般的に行列において$AB \neq BA$である。

\clearpage

### 1-1-3 逆行列

　行列$A$を

$$
A = \begin{pmatrix}
a & b \\
c & d
\end{pmatrix}
\tag{1.1.5}$$

として次の量$A^{-1}$を行列$A$の逆行列と呼び、以下のように定義する。

$$A^{-1} = \frac{1}{ad-bc}\begin{pmatrix}
d & -b \\
-c & a
\end{pmatrix}
\tag{1.1.6}$$

($3\times3$以上の行列の逆行列の求め方は多少複雑なのでここでは割愛。)

### 1-1-4 転置

　ベクトル、行列計算で重要な操作の1つにその行列の「転置」をとるというものがある。
行列$A$の転置を$A^T$として以下のように表す。

$$A^T = \begin{pmatrix}
a & c \\
b & d
\end{pmatrix}\tag{1.1.7}$$

ベクトルの場合は、与えられたベクトルを以下のように横に倒した形で表記すれば良い。(元のベクトルは列ベクトル、それを横に倒したものは行ベクトルと
呼ばれている。)

$$
\vec{a} = \begin{pmatrix}
1  \\
2 \\
3
\end{pmatrix}, \qquad
\vec{a}^T = (1,2,3)
\tag{1.1.8}$$

### 1-1-5 対称行列

　$n \times n$行列（以下は全て$n \times n$行列の議論である）において

$$A=A^{T} \tag{1.1.9}$$

が成立するとき、その行列を対称行列と呼ぶ。


### 1-1-6 行列式

　行列$A$を

$$
A = \begin{pmatrix}
a & b \\
c & d
\end{pmatrix}
\tag{1.1.5}$$

として次の量$det(A)$を行列$A$の行列式と呼び、以下のように定義する。

$$det(A)= ad - bc\tag{1.1.10}$$


### 1-1-7 内積、1次形式、2次形式
　ベクトルの内積を次のように表記する。

$$f = (\vec{a},\vec{x}) \tag{1.1.11}$$

ただし、ベクトル$\vec{a}$, $\vec{x}$は

$$\vec{a} = (a_1,a_2,...,a_n)^T,\qquad \vec{x} = (x_1,x_2,...x_n)^T\tag{1.1.12}$$

とし、$f$を以下のようにする。

$$f = a_1x_1 + \cdots + a_nx_n = \sum_{i=1}^{n}a_ix_i\tag{1.1.13}$$

変数の1次の項のみからなる式を1次形式という。変数の2次の項のみからなる式も
あり、これを2次形式と呼び、以下のようになる。

$$f=a_{11}x_{1}^2+a_{22}x_{2}^{2}+\cdots+a_{nn}x_{n}^2 
+2a_{12}x_{1}x_{2}+2a_{13}x_{1}x_{3}+\cdots+2a_{(n-1)n}x_{n-1}x_{n}\\
=\sum_{i,j=1}^{n}a_{i,j}x_ix_j\tag{1.1.14}$$

これを（1.1.11）のように内積表記すれば、

$$f = (\vec{x},A\vec{x})\tag{1.1.15}$$

となる。ただし$A$は対称行列である。

(1) 任意のベクトル$\vec{x},\vec{y}$,行列$A$に対して、$(A\vec{x},\vec{y})=(\vec{x},A^{T}\vec{y})$ が成立することを示せ。

(略解)
$$(\vec{x},A^T\vec{y})=\sum_{i=1}^nx_i\Bigl(\sum_{j=1}^{n}a_{ji}y_i\Bigl) = \sum_{i,j=1}^{n}a_{ji}x_iy_j=\sum_{j=1}^{n}\Bigl(\sum_{i=1}^{n}a_{ji}x_i\Bigl)y_j=(A\vec{x},\vec{y})\tag{1.1.16}$$

(2) 任意の$n \times n$の行列$A,B$に対して$(AB)^T=B^{T}A^{T}$が成立することを示せ。


### 1-1-8 行列の微分 (重要)
　行列の微分は機械学習で頻繁に登場する。代表的な微分方法として以下の1次形式と2次形式の2つの微分
方法をおさえておけば良い。

$$\frac{\partial}{\partial \vec{w}} \vec{w}^T\vec{x} = (x_1,x_2,...,x_D)^T = \vec{x}\tag{1.1.17}$$
$$\frac{\partial}{\partial \vec{w}}\vec{w}^TA\vec{w} = A\vec{w} + A^T\vec{w} = (A + A^T)\vec{w}\tag{1.1.18}$$

### 1-1-9 線形結合と1次独立

　$c_1,c_2,..,c_n$を定数、$\vec{a}_1,\vec{a}_2,..,\vec{a_n}$を
ベクトルとして、以下の等式、

$$c_1\vec{a}_1+c_2\vec{a}_2+ \cdots +c_n\vec{a}_n = 0\tag{1.1.19}$$

を満たす$c_1,c_2,..,c_n$が、

$$c_1 = c_2 = ... = c_n = 0\tag{1.1.20}$$

のみであるとき、$\vec{a}_1,\vec{a}_2,..,\vec{a_n}$は1次独立である。(そうでない場合は1次従属と呼ばれる。)

### 1-1-10 固有値、固有ベクトル

　ベクトルに行列をかけることは幾何学的にはそのベクトルが回転することに対応する。大抵のベクトルは行列$A$をかけると予期しない方向に回転するが、ここで
行列$A$を作用させてもベクトルの向きが変化しない（$A$に固有な）特別なベクトル
を考える。このベクトルを$\vec{u}$,ベクトルの拡大率を$\lambda$とすると、


$$A\vec{u}=\lambda\vec{u}\tag{1.1.21}$$

とかける。この$\vec{u}$を固有ベクトル、$\lambda$を固有値という。

式（1.1.21）から単純な式変形で以下の方程式、

$$det|A-\lambda I| = 0\tag{1.1.22}$$

を利用すれば$A$の固有値、固有ベクトルが求まることがわかる。

（3）以下の行列$A$の固有値、固有ベクトルを求めよ。

$$A=\begin{pmatrix}
3 & 1 \\
2 & 2
\end{pmatrix}$$

### 1-1-11 行列の対角化 
　行列には「対角化」という操作があり、機械学習にかかわらず工学的に広く
応用されている。
$A$に$n$個の独立な固有ベクトル$\vec{x_1}\cdots\cdots\vec{x_n}$があればそれらは$S$の列に入り、以下のように対角化される。

$$\Lambda=S^{-1}AS\tag{1.1.23}$$


（4）$\Lambda=S^{-1}AS$を利用し、$A^{n}$を求めよ。

(略解)  $(S^{-1}AS)^n = S^{-1}ASS^{-1}AS...S^{-1}ASS^{-1}AS=S^{-1}A^nS,$よって、$A^{n} = S(S^{-1}AS)^nS^{-1}=S\Lambda^nS^{-1}$

　上記問で、対角化を用いて$A^{n}$を求めたが、これは行列微分方程式や、
フーリエ変換と行った工学上重要な分野で利用されている。


### 1-1-12 対称行列の対角化

　固有値が重複することによって固有ベクトルが足りなくなることがある。このとき
行列$A$の対角化は不可能になってしまう。しかし、対称行列の場合には、必ず
対角化することが可能であり、$A$の要素が全て実数（実対称行列）なら、

1. 固有値が全て実数
2. 固有ベクトルも（都合がいいことに）必ず全て直交

という性質を持つ。

(5) 実対称行列の全ての固有値が実数であることを示せ。

(略解) $A\vec{x} = \lambda\vec{x}$の両辺の複素共役をとり、$A\bar{\vec{x}} = \bar{\lambda}\bar{\vec{x}}$、これを転置して$\bar{\vec{x}}^TA = \bar{\vec{x}}^T\bar{\lambda}$。$A\vec{x} = \lambda\vec{x}$に対して$\bar{\vec{x}}$との内積、$\bar{\vec{x}}^TA = \bar{\vec{x}}^T\bar{\lambda}$に対して$\vec{x}$との内積を考えると、

$$\bar{\vec{x}}^TA\vec{x} = \bar{\vec{x}}^T\lambda\vec{x},\qquad\bar{\vec{x}}^TA\vec{x} = \bar{\vec{x}}^T\bar{\lambda}\vec{x}\tag{1.1.24}$$

よって$\lambda = \bar{\lambda}$


(6) 実対称行列の異なる$\lambda$に対応する固有ベクトルは
必ず直交することを示せ。

(略解) $A\vec{x} = \lambda_1\vec{x},A\vec{y} = \lambda_2\vec{y}$とし、$\lambda_1\neq\lambda_2$とする。

$$(\lambda_1\vec{x})^T\vec{y} = (A\vec{x})^T\vec{y} = \vec{x}^{T}A^{T}\vec{y} = \vec{x}^TA\vec{y}=\vec{x}^T\lambda_2\vec{y}\tag{1.1.25}$$

よって、$\vec{x}^T\lambda_1\vec{y}=\vec{x}^T\lambda_2\vec{y},\lambda_1\neq\lambda_2$より、$\vec{x}^T\vec{y}=0$

　2.の性質を利用し、固有ベクトルを正規直交するように定数倍すると、
(1.1.23)の対角化をさらに簡単に行うことができる。

（スペクトル定理）全ての対称行列は、実数固有値からなる$\Lambda$と
正規直交する固有ベクトルからなる$S=Q$を用いて

$$A=Q\Lambda Q^{T}\tag{1.1.26}$$

と分解される。ここで$Q$は直交行列と呼ばれ、以下の性質を満たす。
（これは物理、工学の分野では主軸定理と呼ばれている。）

$$q_iq_j=\delta_{i,j}\tag{1.1.27}$$

$$
    \delta_{i,j} =
        \begin{cases}
            1 & i = j \\
            0 & i \neq j
        \end{cases}\tag{1.1.28}
$$

$$Q^{-1} = Q^{T}\tag{1.1.29}$$

これらの性質から対称行列は工学上よく利用される。


一部文献では、以下のような表記も行われることがある。

$$A = \lambda_1\vec{x_1}\vec{x_1}^T + \lambda_1\vec{x_1}\vec{x_1}^T + \cdots + \lambda_n\vec{x_n}\vec{x_n}^T\tag{1.1.30}$$


（注意）固有値が重複した場合、固有ベクトルが足りず対角化不可能のように思われるが、
対称行列においては重複した固有値にはその重複度分の独立な固有ベクトルが存在することが
保証される。（証明割愛）これらは互いに直交していないことが多いが、
シュミットの直交化を用いて、独立なベクトルを正規直交するベクトルに
取り直すことができるので、結局全ての固有ベクトルを直交するようにとる
ことができる。

(補足) 対称行列以外の行列は固有値が重複した場合、固有ベクトルが足りず対角化不可能な場合がある。その場合は、「ジョルダン標準形」と呼ばれる可能な
限り対角行列に近い行列を作るというアプローチをとるのが一般的である。

### 1-1-13 2次形式の標準形
　２次形式で表される2変数の2次関数

$$ax^2+2bxy+cy^2=1\tag{1.1.31}$$

を考える。これは（傾いた）楕円を表す。$x,y$に行列$U$を作用し、

$$\vec{x}=U\vec{\tilde{x}}\tag{1.1.32}$$

とする。このとき2次形式の内積と対称行列の対角化を利用すると以下のように
なる。

$$(\vec{x},A\vec{x})=(U\vec{\tilde{x}},AU\vec{\tilde{x}})=(\vec{\tilde{x}},U^{T}AU\vec{\tilde{x}}) = (\vec{\tilde{x}},\Lambda\vec{\tilde{x}}) \\
= \lambda_1\tilde{x_1}^2+\lambda_2\tilde{x_2}^2+\cdots+\lambda_n\tilde{x_n}^2\tag{1.1.33}$$

これを2次形式の標準形と呼ぶ。これは元々の楕円$(\vec{x},A\vec{x})$を回転させ、新しい軸の方向に長軸、短軸をとることが
できることを意味している。

　$\lambda_1,\lambda_2,\cdots\lambda_n$が正の値をとるとき、
$(\vec{x},A\vec{x}) > 0$となる。固有値が全て正の値をとる行列$A$の
ことを正定値行列と呼び、このとき行列$A$は、$(\vec{x},A\vec{x}) > 0$
を満たす。またこの形式の2次曲面、

$$f(x,y)=ax^2+2bxy+cy^2\tag{1.1.34}$$

を考えると（1.1.34）より、

$$(\vec{x},A\vec{x})= \lambda_1\tilde{x_1}^2+\lambda_2\tilde{x_2}^2\tag{1.1.35}$$

であり、$\lambda_1 > 0,\lambda_2 > 0$のとき、この2次曲面は唯一の
最小値を持つはずである。このことから、2次曲面が最小値（または最大値）を
持つか持たないか（これはたびたび機械学習の分野で話題になる）は、
2次形式$(\vec{x},A\vec{x})$で式を表した際の行列$A$が正定値行列か（
もしくは負定値行列か）どうかを求めれば判定することができる。

## 1-2 重回帰と最小二乗法

入力をD次元ベクトル$\vec{x}^T = (1,x_1,x_2,...,x_D)$, 出力を$y$とすると

$$y = w_0 + w_1x_1 + w_2x_2 + \cdots + w_Dx_D = \vec{w}^T\vec{x} \tag{1.2.1}$$

の$w_0,w_1,..w_D$を求める問題を重回帰と呼ぶ。式(1.2.1)を$N$個のデータについて並べて、

$$\vec{\hat{y}} = \begin{pmatrix}
\hat{y}_1  \\
\hat{y}_2 \\
\vdots \\
\hat{y}_n \\
\vdots \\
\hat{y}_N
\end{pmatrix} = \begin{pmatrix}
\vec{w}^Tx_1  \\
\vec{w}^Tx_2 \\
\vdots \\
\vec{w}^Tx_n \\
\vdots \\
\vec{w}^Tx_N
\end{pmatrix} = \begin{pmatrix}
\vec{x}_1^T  \\
\vec{x}_2^T \\
\vdots \\
\vec{x}_n^T \\
\vdots \\
\vec{x}_N^T
\end{pmatrix} \vec{w} = X\vec{w} \tag{1.2.2}$$

と表記することができる。($X$は計画行列と呼ばれている。)正解データ$\vec{y_n}$との誤差を考えると、

$$E = \sum_{n=1}^N(y_n - \hat{y_n})^2 = \sum_{n=1}^{N}(y_n - \vec{w}^T\vec{x}_n)^2 \tag{1.2.3}$$

この誤差$E$は「最小二乗誤差」と呼ばれ最も標準的な誤差関数である。(ほとんどの機械学習のモデルではこういった誤差関数という類の関数を最小にすることを「学習」と呼んでいる。)この誤差関数を$\vec{w}$について微分すると、

$$\frac{\partial E}{\partial \vec{w}} = 0$$
$$\Longrightarrow X^TX\vec{w} = X^T\vec{y} \tag{1.2.4}$$

式(1.2.4)は重回帰モデルの「正規方程式」と呼ばれている。

$X^TX$が正則なとき上記方程式の解は

$$\vec{w} = (X^TX)^{-1}X^T\vec{y} \tag{1.2.5}$$

となる。これが重回帰における重みの値となる

(1) 式(1.2.5)の正規方程式を導け。


## $Column\sim$ リッジ回帰と正則化 $\sim$

重回帰の正規方程式は、

$$\vec{w} = (X^TX)^{-1}X^T\vec{y} \tag{C.1.2.1}$$

であることをみた。この式についてもう少し考える。

今、$X^TX$に注目すると、この行列は正方行列となっている。$D\times D$正方行列であるとすると、式($C.1.2.1$)はこの正方行列の逆行列を含んでいることがわかる。しかし、一般的にある行列が逆行列を持つことは自明ではないし、逆行列を持っていたとしても計算が不安定になる場合がある。次の例をみてみよう。

$$X = \begin{pmatrix}
1 & 2 & 4 \\
1 & 3 & 6.1 \\
1 & 4 & 7.9
\end{pmatrix}\tag{C.1.2.2}$$

この場合、$(X^TX)^{-1}$を考えると以下のようになる。

$$(X^TX)^{-1} = \begin{pmatrix}
6.33 & 18.00 & -10.00 \\
18.00 & 254.00 & -130.00 \\
-10.00 & -130.00 & 66.67
\end{pmatrix}\tag{C.1.2.3}$$

このように$D\times D$正方行列$(X^TX)^{-1}$の各要素は絶対値が非常に大きくなってしまうことがわかる。式$(C.1.2.1)$を考えると、これによって計算
される$\vec{w}$も当然大きくなることが予想される。図1,2(1-2_Column図表.pdf参照)を見るとわかるように
重み$\vec{w}$の要素が大きくなればなるほど、予測値がデータの違いに過敏に
反応するようになり、正しい学習結果を得ることができない。このような状況を
機械学習の分野では、「過学習」と呼ぶ。

この過学習を防ぐのによく使われる手法として、「正則化」というものがある。これは誤差関数$E$に係数ベクトルの大きさ$|\vec{w}|^2 = \vec{w}^T\vec{w}$を加えた以下の式を最小化する。

$$E = (\vec{y} - X\vec{w})^T(\vec{y} - X\vec{w}) + \alpha \vec{w}^T\vec{w},\qquad \alpha \geq 0\tag{C.1.2.4}$$

この式から正規方程式を導くと、

$$(X^TX + \alpha I)\vec{w} = X^T\vec{y}\tag{C.1.2.5}$$
$$\vec{w} = (X^TX + \alpha I)^{-1}X^T\vec{y}\tag{C.1.2.6}$$

となる。すると、$(X^TX + \alpha I)^{-1}$は先ほどの例でいくと、$\alpha = 0.1$として、

$$(X^TX + 0.1 \times I)^{-1} = \begin{pmatrix}
3.34 & 0.16 & -0.60 \\
0.16 & 7.70 & -3.88 \\
-0.60 & -3.88 & 2.04
\end{pmatrix} \tag{C.1.2.7}$$

となって逆行列の値が$+ \alpha I$の項によって安定していることがわかる。式$(C.1.2.6)$のような係数の求め方を回帰分析の分野では「リッジ回帰」と呼ぶ。

また、次にように完全に列の定数倍になっている場合、

$$X = \begin{pmatrix}
1 & 2 & 4 \\
1 & 3 & 6 \\
1 & 4 & 8
\end{pmatrix} \tag{C.1.2.8}$$

を考える。この場合は、$D\times D$正方行列$X^TX$のランクが$\rm rank(X^TX) \leq D$となり、ランク落ちしてしまうので、逆行列の計算がそもそもできない。(説明変数の数$D$が、データ点の数$N$より多い(背が低く幅広な行列)ことが原因でランク落ちすることもある。)こういう場合も、リッジ回帰を使えば、$+\alpha I$の項のおかげで行列の退化(ランクが足りないこと)を防げ、逆行列を計算することが可能になる。なお、リッジ回帰を使わない場合は、「一般化逆行列」というものを使って本来計算できない
行列の逆行列を計算することができるが、この場合も結果は不安定なものに陥りやすい。(一般化逆行列はさらに発展的な内容なため割愛。)


(補足)
式$(C.1.2.2)$のデータは3列目が2列目の(ほぼ)定数倍になっている。このように
考えている$x_1,x_2,...$(回帰分析の分野では説明変数と呼ぶ)の間に強い
相関がある場合、学習をうまく行うことができず不安定な結果に陥る。このような
状況は「多重共線性」と呼ばれている。このような状況は、説明変数をむやみやたらに増やすと当然起こりやすくなるので、多項式の項数や次数を増やす際には注意
が必要である。


## 1-3 パーセプトロン
　線形識別関数$f(x)=w^Tx$を用いて、$f(x) \geq 0$のとき$x \in C_1$(クラス1)、$f(x) < 0$のとき$x\in C_2$(クラス2)のようにする分類問題を考える。パーセプトロンではまず、重回帰と同じようにデータ$\vec{x_i}$と重み$\vec{w}$の間の線形結合を考え、以下のような計算を行う。

$$z_i =  w_0 + w_1x_1 + w_2x_2 + \cdots + w_Dx_D = \vec{w}^T\vec{x}_i \tag{1.2.6}$$

この計算結果$z_i$にしたがって、$i$番目のデータに対して次のようなラベル付けを行う。(これがパーセプトロンの識別規則である。)

$$
    f(\vec{x_i}) =
        \begin{cases}
            1 & z_i \geqq 0 \\
            0 & z_i < 0 \\
        \end{cases}\tag{1.2.7}
$$

パーセプトロンの学習規則は$i$番目のデータ$x_i$を入力
したときの出力$f(x_i)$に応じて以下のようになる。

$$
    \vec{w}_{i+1} =
        \begin{cases}
            \vec{w_i} & f(\vec{x_i}) \geqq 0 \\
            \vec{w_i}+\eta \vec{x_i} & f(\vec{x_i}) < 0 \\
        \end{cases}\tag{1.2.8}
$$

(注意、片側のクラスに属するデータの符号を反転させるとどちらのクラスの
データも超平面の同じ側にすることができるので、どちらのクラスに属していても、分類が正しければ$f(x) \geq 0$、間違えていれば$f(x) < 0$になる。)

また、パーセプトロンは2クラスの学習データが線形分離可能であれば、有限の学習回数$M$で収束することが保証され
ている。(パーセプトロンの収束定理)

(2) パーセプトロンの収束定理

$$M\frac{D^2(\vec{w^*})\eta}{d(\eta+2\alpha)}\leq\phi\leq1 \Longrightarrow M \leq d\frac{1+2\alpha/\eta}{D_{max}^2}$$

  を証明せよ。

## 1-4 ロジスティック回帰

　パターン認識を行う際、単に分類結果だけではなく、「クラス$C_1$に属する確率70%、クラス$C_2$に属する確率30%」といった確率値を出力してくれた方が
便利である。これを実現する手法で代表的なものが「ロジスティック回帰」である。

### 1-4-1 一般化線形モデル

　ロジスティック回帰を考える前に、1-2節でも扱った以下の線形モデル

$$y = \vec{w}^T\vec{x} \tag{1.4.1}$$

を考える。左辺の予測したい値$y$は「応答変数」、右辺は「線形予測子」と呼ばれている。応答変数が正規分布であるような問題を考える場合はこの定式
で十分であるが、応答変数が非負の実数である場合や離散変数である場合は(例として気温や、カテゴリーデータなど)、右辺の線形予測子との対応関係が取れないため、意味のある出力結果が得られない。こういった場合には、式(1.4.1)に数学的な操作を
行って両辺の対応関係がうまくとれるように変形することができる。

例として、応答変数$y$が非負の実数である場合、

$$\ln(y) = \vec{w}^T\vec{x} \tag{1.4.2}$$

とすれば、$0 < \ln(y) < \infty$であるため、両辺の対応関係がとれる。

両辺の対応関係をとるために用いる関数は「リンク関数」と
呼ばれている。このようにして、応答変数が正規分布以外の分布に従うものに
対して線形モデルを拡張したものを「一般化線形モデル」と呼ぶ。

### 1-4-2 ロジット関数とロジスティック回帰

　出力結果を確率値$(0,1)$にするためには、以下のようにリンク関数にロジット関数を適用すれば良い。

$$\ln\Bigl(\frac{p}{1-p}\Bigl) = \vec{w}^T\vec{x} \tag{1.4.3}$$

これを$p$について解くと、

$$p = \frac{1}{1+\exp(\vec{w}^T\vec{x})} \tag{1.4.4}$$

(回帰という名前がついていて紛らわしいが、線形関数を$(0,1)$の間に回帰する
という意味で、ロジスティック回帰自体は分類に使われる。)

### 1-4-3 最尤推定

　2クラス分類を考える。n番目のデータをロジスティック回帰に適用したときに
得られる出力確率$P_n$は、教師データ(ラベル変数)$t_n$を用いて以下のように
表せる。(このような分布はベルヌーイ分布と呼ばれている。)

$$P_n = p_n^{t_n}(1-p_n)^{1-t_n} \tag{1.4.5}$$

この$P_n$を学習する全データについて計算して掛け合わせたものを考え、これを$L(w)$とすると、

$$L(w) = P_1P_2\cdots P_N = \prod_{n=1}^{N}p_n^{t_n}(1-p_n)^{1-t_n} \tag{1.4.6}$$

この$L(w)$は「尤度関数」と呼ばれている。(尤度、最尤推定についてはColumn参照。)この尤度関数を最大化するような
$w$を求めることが、ロジスティック回帰における「学習」である。実際は、
数値計算の観点からこの尤度関数ではなく、次の負の対数尤度関数、

$$E(w) = -\ln L(w) = -\sum_{n=1}^{N}(t_n\ln p_n + (1-t_n)\ln(1-p_n)) \tag{1.4.7}$$

を最小化する。この$E(w)$を「交差エントロピー誤差関数」と呼び、$E(w)$を
最小とするパラメータ$w$は勾配降下法といった最適化手法が用いられる。

(1) 次のシグモイド関数を微分しなさい。

$$p=\frac{1}{1 + \exp(-y)}$$

(2) $E(w)$を$w_i$($i$番目の特徴量)で微分して以下を示せ。

$$\frac{\partial E(w)}{\partial w_i} = \sum_{n=1}^{N}(p_n-t_n)x_n^{(i)} \tag{1.4.8}$$

この講習会では、$n$番目のデータの特徴量$i$(計画行列$X$の$(n,i)$成分)を
$x_n^{(i)}$と表記することにする。

## 1-5 SVM（サポートベクトルマシン）

### 1-5-1 SVMの主問題

次のようなクラス分類の問題を考える。

$$t_i(\vec{w}^T\vec{x}_i + b) > 0, \quad t_i = 1,2,...n, \quad t_{i} =
        \begin{cases}
            1 & x_i \in K_1 \\
            -1 & x_i \in K_2 \\
        \end{cases} \tag{1.5.1}$$

この問題を解くために、p次元データ$\vec{x_i} = (x_1,x_2,..., x_p)$と超平面$\vec{w}^T\vec{x}_i + b = 0$の距離を

$$d = \frac{|w_1x_1+w_2x_2+\cdots + w_px_p+b|}{\sqrt{w_1^2+w_2^2+...w_p^2}} = \frac{|\vec{w}^T\vec{x}_i + b|}{||\vec{w}||} \tag{1.5.2}$$


とし、2つのクラスを分ける超平面とそれに最も近いデータ(サポートベクトル)との
間の距離(マージン$M$)を最大化するように$w,b$を最適化する。

$$argmax_{\vec{w},b} M,\quad \frac{t_i(\vec{w}^T\vec{x}_i + b)}{||\vec{w}||} \geq M,\quad i=1,2,...n\tag{1.5.3}$$

これに簡単な式変形を加えると次のようにこの最適化問題を書き換えることができる。

$$ argmin_{\vec{w},b}\frac{1}{2}||\vec{w}||^2,\quad t_i(\vec{w}^T\vec{x}_i+b)\geq 1, \quad i=1,2,...n\tag{1.5.4}$$

また、線形分離可能でない場合にスラッグ変数$\epsilon_i$を導入してこの最適化問題の制約を緩めることができる。

$$argmin_{\vec{w},b}\Bigl\{\frac{1}{2}||\vec{w}||^2 + C\sum_{i=0}^{n}\epsilon_i\Bigl\},\quad t_i(\vec{w}^T\vec{x}_i+b)\geq 1-\epsilon_i,\quad \epsilon_i \geq 0 \quad i=1,2,...n \tag{1.5.5}$$
$$\epsilon_i = max\biggl\{0, M - \frac{t_i(\vec{w}^T\vec{x}_i + b)}{||\vec{w}||}\biggl\}$$

これはソフトマージンSVMと呼ばれている。またこれらの最適化問題は、不等式制約条件最適化問題の「主問題」と
呼ばれており、主問題に対して「双対問題」という形式の問題を導くことができる。

(1)式(1.5.3)から式(1.5.4)への式変形をせよ。


### 1-5-2 SVMの双対問題と非線形分離

　ラグランジュ未定乗数法を用いると、上記の主問題を双対問題に変形することができる。

$$argmax_{\alpha}\Biggl\{L(\alpha) = \sum_{i=1}^{n}\alpha_i - \frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\alpha_jt_it_j\vec{x}_i^T\vec{x}_j\Biggl\} \tag{1.5.6}$$
$$\sum_{i=1}^{n}\alpha_it_i= 0 ,\quad 0 \leq \alpha_i \leq C, \quad i = 1,2,...n$$

学習データが線形識別関数で分離できない場合は、高次元非線形空間にデータ点を
写像し、その空間内で線形識別関数を用いると、線形分離可能となる可能性がある。主問題を双対問題に変形しておくと、学習データの高次元非線形空間への写像は単に

$$argmax_{\alpha}\Biggl\{L(\alpha) = \sum_{i=1}^{n}\alpha_i - \frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\alpha_jt_it_j\Phi(\vec{x})_i^T\Phi(\vec{x})_j\Biggl\} \tag{1.5.7}$$
$$\sum_{i=1}^{n}\alpha_it_i= 0 ,\quad 0 \leq \alpha_i \leq C, \quad i = 1,2,...n$$

と問題を変えるだけでよいので定式化が容易であり、さらにカーネル法(次節参照)を適用できる形式になる。

(2)式(1.5.5)の最適化問題にラグランジュ未定乗数法を適用することで得られる以下の
ラグランジュ関数

$$L(\vec{w},b,\epsilon,\alpha,\beta) = \frac{1}{2}||\vec{w}||^2 + C\sum_{i=1}^{n}\epsilon_i - \sum_{i=1}^{n}\alpha_i\{t_i(\vec{w}^T\vec{x}_i + b)-1 + \epsilon_i\}-\sum_{i=1}^{n}\beta_i\epsilon_i \tag{1.5.8}$$

　について、主変数$w,b,\epsilon$の偏微分を考え、双対問題(1.5.6)を導け。


### 1-5-3 カーネル法

　双対問題に現れる値$\Phi(\vec{x})_i$は計算量が多く最適化問題を解くことが困難な形に
なってしまっているが、$\Phi(\vec{x})_i$の内積$\Phi(\vec{x})_i^T\Phi(\vec{x})_j$は
$\Phi(\vec{x})_i$を計算しないで求めることができることが知られている。

$$K(\vec{x}_i, \vec{x}_j) = \Phi(\vec{x})_i^T\Phi(\vec{x})_j \tag{1.5.9}$$

$K(\vec{x}_i, \vec{x}_j)$をカーネル関数と呼び、このようにして内積を計算する手法を「カーネル法」とも呼ぶ。(これを決めてしまえば内積計算$\Phi(\vec{x})_i^T\Phi(\vec{x})_j$を簡単に行うことができてしまうため、この一見魔法のよう
な手法は「カーネルトリック」も呼ばれている。)

以下にいくつかのカーネル関数をあげる。(実際は、解きたい問題に合わせてこれらを
使い分ける。)

ガウスカーネル

$$K(\vec{x}_i, \vec{x}_j) = \exp\Biggl\{-\frac{||\vec{x}_i - \vec{x}_j||^2}{2\sigma^2}\Biggl\} \tag{1.5.10}$$

多項式カーネル

$$K(\vec{x}_i, \vec{x}_j) = (\vec{x}_i^T\vec{x}_j + c)^d \tag{1.5.11}$$

シグモイドカーネル

$$K(\vec{x}_i, \vec{x}_j) = tanh(b\vec{x}_i^T\vec{x}_j + c) \tag{1.5.12}$$

(3)以下のような2次元入力空間から、3次元特徴空間への写像を考える。

$$\Phi(\vec{x}) = (x_1^2, \sqrt{2}x_1x_2, x_2^2) \tag{1.5.13}$$

このときこれらの内積$\Phi(\vec{x})^T\Phi(\vec{y})$が、$c=0,d=2$とした多項式カーネル、

$$(\vec{x}^T\vec{y})^2 \tag{1.5.14}$$

で表されることを示せ。

　上記の問題(3)のように、例えば多項式カーネルでは、$d$次以下の全種類の単項式を各成分に持つような特徴ベクトル$\Phi(x)$の内積を求めていることに対応する。(カーネルを決めたら自動的に写像のされ方が決定するので、問題に合わせてカーネルを選定する必要がある。)

## 1-6 教師なし学習

　教師データを使わずに学習を行うことを「教師なし学習」と呼ぶ。教師なし学習は、
主にクラスタリングの用途で使われ、データに隠れている構造を発見したり、
教師あり学習の前処理として用いられる。

### 1-6-1 k-meansアルゴリズム

　クラスタリング(教師なし学習)によく使われる手法に「K-meansアルゴリズム」がある。
K-meansアルゴリズムでは以下の目的関数を最小化する。

$$J = \sum_{n=1}^{N} \sum_{k=1}^{K} r_{nk}||\vec{x}_n - \vec{\mu}_n||^2 \tag{1.6.1}$$

$\vec{\mu}_k$について解くと、

$$\vec{\mu}_k = \frac{\sum_nr_{nk}\vec{x}_n}{\sum_nr_{nk}} \tag{1.6.2}$$

　この式の分母は$k$番目のクラスタに割り当てられたデータの数に等しいので、$\vec{\mu}_k$は、
$k$番目のクラスタに割り当てられた全てのデータ点$\vec{x_n}$の平均となっている。これがK-meansアルゴリズムと呼ばれている理由である。
(なお、K-meansアルゴリズムは次章の混合ガウス分布に対するEMアルゴリズムの非確率的
極限となっている。)

### 1-6-2 次元削減

　教師なし学習の応用の一つに「次元削減」があげられる。学習するパラメータ
の数を減らすことができるため、計算時間を減らすために主に用いられる。

　次元削減は数学的に見れば、データ空間でより次元が低い部分空間にデータ点を
射影することで実現される。しかしその過程でデータが元々持っていた情報が
失われてしまうため、できるだけ情報を残しながらデータ空間の次元を下げる
必要がある。これを実現するための手法として、データの持つ分散(ばらつき)
が大きくなるようにデータを射影するものがあり、これは「主成分分析」と呼ばれる。

　学習用データを表す行列を$X(N\times M)$とおく。このときデータの共分散行列は以下の
ように表すことができる。

$$\Sigma = \frac{1}{N}X^TX \tag{1.6.3}$$

　ある特定の方向をもつ単位ベクトル$\vec{e}$を考える。このときこの単位ベクトルにデータ点を射影してできる新しいデータ点に対して分散$\sigma$を考えると、

$$\sigma = \vec{e}^T\Sigma\vec{e} \tag{1.6.4}$$

となる。この$\sigma$に対して、ラグランジュ未定乗数法を適用して$\sigma$
の最大値を求める(分散最大化)をしようとすると、以下の最適化問題を解く
ことになる。

$$argmax_{\vec{e}}\Biggl\{L(\vec{e},\lambda) = \sigma - \lambda(\vec{||e||^2} -1)\Biggl\} , \quad \vec{||e||^2} = 1 \tag{1.6.5}$$

この最適化問題を解くと主成分分析は以下の固有値問題、

$$\Sigma \vec{e} = \lambda \vec{e} \tag{1.6.6}$$

となる。分散が最大になるような単位ベクトル$\vec{e}$を「第一主成分」と呼び、
上記の固有値問題のうち、固有値が最大になる固有ベクトルが第一主成分
に対応する。

(1)3つの2次元データ$\vec{a_1},\vec{a_2},\vec{a_3}$を考え、主成分分析が式(1.6.6)の固有値問題で定式化されることを確認せよ。
 
(略解) $\vec{a_1},\vec{a_2},\vec{a_3}$を並べた3×2行列$X$を考え、

$$\Sigma = \frac{1}{3}X^TX$$

が成立することを確認する。次に2次元単位ベクトル$\vec{e}=(e_x,e_y)^T$
を考え、$\vec{a_1},\vec{a_2},\vec{a_3}$それぞれとの内積を考えること
で分散$\sigma$を求めて式(1.6.4)を示す。最後にこの$\sigma$を用いて式(1.6.5)を
構成してラグランジュ未定乗数法を用いて以下の値を計算してその結果を用いて
固有方程式(1.6.6)を導出する。(補足　2次形式の標準形と最大最小の議論(1-1-13節)について知っていれば、
式(1.6.4)を導出した時点で直ちに固有値を求めに行ってもよい。)

$$\frac{\partial L(\vec{e},\lambda)}{\partial \vec{e}},\quad\frac{\partial L(\vec{e},\lambda)}{\partial \lambda}$$

