# Pythonを用いた初心者向けAI実践講座(中級編) 10/26 配布資料

## 1-4 ロジスティック回帰

　パターン認識を行う際、単に分類結果だけではなく、「クラス$C_1$に属する確率70%、クラス$C_2$に属する確率30%」といった確率値を出力してくれた方が
便利である。これを実現する手法で代表的なものが「ロジスティック回帰」である。

### 1-4-1 一般化線形モデル

　ロジスティック回帰を考える前に、1-2節でも扱った以下の線形モデル

$$y = w^Tx \tag{1.4.1}$$

を考える。左辺の予測したい値$y$は「応答変数」、右辺は「線形予測子」と呼ばれている。応答変数が正規分布であるような問題を考える場合はこの定式
で十分であるが、応答変数が非負の実数である場合や離散変数である場合は(例として気温や、カテゴリーデータなど)、右辺の線形予測子との対応関係が取れないため、意味のある出力結果が得られない。こういった場合には、式(1.4.1)に数学的な操作を
行って両辺の対応関係がうまくとれるように変形することができる。

例として、応答変数$y$が非負の実数である場合、

$$\ln(y) = w^Tx \tag{1.4.2}$$

とすれば、$0 < \ln(y) < \infty$であるため、両辺の対応関係がとれる。

両辺の対応関係をとるために用いる関数は「リンク関数」と
呼ばれている。このようにして、応答変数が正規分布以外の分布に従うものに
対して線形モデルを拡張したものを「一般化線形モデル」と呼ぶ。

### 1-4-2 ロジット関数とロジスティック回帰

　出力結果を確率値$(0,1)$にするためには、以下のようにリンク関数にロジット関数を適用すれば良い。

$$\ln\Bigl(\frac{p}{1-p}\Bigl) = w^Tx \tag{1.4.3}$$

これを$p$について解くと、

$$p = \frac{1}{1+\exp(w^Tx)} \tag{1.4.4}$$

(回帰という名前がついていて紛らわしいが、線形関数を$(0,1)$の間に回帰する
という意味で、ロジスティック回帰自体は分類に使われる。)

### 1-4-3 最尤推定

　2クラス分類を考える。n番目のデータをロジスティック回帰に適用したときに
得られる出力確率$P_n$は、教師データ(ラベル変数)$t_n$を用いて以下のように
表せる。(このような分布はベルヌーイ分布と呼ばれている。)

$$P_n = p_n^{t_n}(1-p_n)^{1-t_n} \tag{1.4.5}$$

この$P_n$を学習する全データについて計算して掛け合わせたものを考え、これを$L(w)$とすると、

$$L(w) = P_1P_2\cdots P_N = \prod_{n=1}^{N}p_n^{t_n}(1-p_n)^{1-t_n} \tag{1.4.6}$$

この$L(w)$は「尤度関数」と呼ばれている。(尤度、最尤推定についてはColumn参照。)この尤度関数を最大化するような
$w$を求めることが、ロジスティック回帰における「学習」である。実際は、
数値計算の観点からこの尤度関数ではなく、次の負の対数尤度関数、

$$E(w) = -\ln L(w) = -\sum_{n=1}^{N}(t_n\ln p_n + (1-t_n)\ln(1-p_n)) \tag{1.4.7}$$

を最小化する。この$E(w)$を「交差エントロピー誤差関数」と呼び、$E(w)$を
最小とするパラメータ$w$は勾配降下法といった最適化手法が用いられる。

(1) $E(w)$を$w$で微分して以下を示せ。

$$\frac{\partial E(w)}{\partial w} = \sum_{n=1}^{N}x_n(y_n-t_n) \tag{1.4.8}$$